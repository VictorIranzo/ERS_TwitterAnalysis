{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ejercicio de twitter.\n",
    "Entrega: 25-mar-2020 23:55\n",
    "\n",
    "Encontrar los 5 estados más felices de USA de acuerdo al análisis de sentimiento con AFFINN. \n",
    "\n",
    "Contenido. un fichero .ipynb con enunciado. Si no se hace completo indicad en un comentario lo que se ha conseguido Mostrar mapa USA con los estados coloreados por felicidad\n",
    "\n",
    "Realizar un mapa de España que permita ver dónde se tuitea y en qué idioma se tuitea\n",
    "\n",
    "Subir fichero ipynb y fichero html!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "states = [\"ak\",\"al\",\"ar\",\"az\",\"ca\",\"co\",\"ct\",\"de\",\"fl\",\"ga\",\"hi\",\"ia\",\"id\",\"il\",\n",
    "          \"in\",\"ks\",\"ky\",\"la\",\"ma\",\"md\",\"me\",\"mi\",\"mn\",\"mo\",\"ms\",\"mt\",\"nc\",\"nd\",\"ne\",\"nh\",\n",
    "          \"nj\",\"nm\",\"nv\",\"ny\",\"oh\",\"ok\",\"or\",\"pa\",\"ri\",\"sc\",\"sd\",\"tn\",\"tx\",\"ut\",\"va\",\"vt\",\n",
    "          \"wa\",\"wi\",\"wv\",\"wy\"]\n",
    "\n",
    "states_names = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', 'CO': 'Colorado',\n",
    "'CT': 'Connecticut', 'DE': 'Delaware', 'DC': 'District of Columbia', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky',\n",
    "'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota',\n",
    "'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
    "'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC':'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
    "'TN':'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington','WV': 'West Virginia',\n",
    "'WI': 'Wisconsin', 'WY': 'Wyoming', 'PR': 'Puerto Rico'}\n",
    "\n",
    "states_codes = {'AL': 1, 'AK': 2, 'AZ': 4, 'AR': 5, 'CA': 6, 'CO': 8,\n",
    "'CT': 9, 'DE': 10, 'DC': 11, 'FL': 12, 'GA': 13, 'HI': 15, 'ID': 16, 'IL': 17,\n",
    "'IN': 18, 'IA': 19, 'KS': 20, 'KY': 21, 'LA': 22, 'ME': 23, 'MD': 24,\n",
    "'MA': 25, 'MI': 26, 'MN': 27, 'MS': 28, 'MO': 29, 'MT': 30,\n",
    "'NE': 31, 'NV': 32, 'NH': 33, 'NJ': 34, 'NM': 35, 'NY': 36, 'NC':37, 'ND': 38, 'OH': 39, 'OK': 40,\n",
    "'OR': 41, 'PA': 42, 'RI': 44, 'SC': 45, 'SD': 46, 'TN':47,\n",
    "'TX': 48, 'UT': 49, 'VT': 50, 'VA': 51,\n",
    "'WA': 53,'WV': 54, 'WI': 55, 'WY': 56, 'PR': 72}\n",
    "\n",
    "afinnfile = open(\"AFINN-111.txt\")\n",
    "afinnDict = {}\n",
    "\n",
    "for line in afinnfile:\n",
    "    term, score  = line.split(\"\\t\")\n",
    "    afinnDict[term] = int(score)\n",
    "\n",
    "def getState(data):\n",
    "    if data[\"place\"] != None and data[\"place\"][\"country_code\"] == \"US\":\n",
    "        state = str(data[\"place\"][\"full_name\"]).lower().split(\", \")\n",
    "        \n",
    "        if len(state) > 1:\n",
    "            return state[1]\n",
    "\n",
    "def isState(state):\n",
    "    if state in states:\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "def analyzeEmotions(tweet):\n",
    "    termsCount = 0\n",
    "    score = 0\n",
    "\n",
    "    for word in tweet:\n",
    "        strippedWord = word.lower().strip().replace('.','').replace('?','').replace('!','')\n",
    "\n",
    "        if strippedWord in afinnDict.keys():\n",
    "            termsCount += 1\n",
    "            score += afinnDict[strippedWord]\n",
    "        \n",
    "    return termsCount, score\n",
    "\n",
    "def analyzeUsa():\n",
    "    file = \"inp.txt\"\n",
    "    scoredTweets = []\n",
    "    tweetsByStates = {}\n",
    "       \n",
    "    with open(file, \"r\", encoding='utf-16') as ins:\n",
    "        \n",
    "        for line in ins:          \n",
    "            line = line.strip(\"'<>() \").replace('\\'', '\\\"')\n",
    "            line = line.replace(\"b\\\"{\", \"{\")\n",
    "            line = line.replace(\"}\\\"\", \"}\")\n",
    "            line = line.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        \n",
    "            if len(line) > 1: ## to avoid empty lines \n",
    "                tweet = json.loads(line, encoding=\"utf-16\")\n",
    "           \n",
    "                if \"created_at\" in tweet:\n",
    "                    state = getState(tweet)\n",
    "\n",
    "                    if isState(state):\n",
    "                        if \"text\" in tweet:                           \n",
    "                            tweetText = tweet[\"text\"]\n",
    "                            tokenizedTweet = nltk.word_tokenize(tweetText)\n",
    "                            \n",
    "                            score = 0\n",
    "                            numberTerms = 0\n",
    "                            \n",
    "                            numberTerms, score = analyzeEmotions(tokenizedTweet)\n",
    "                            \n",
    "                            if numberTerms > 0:\n",
    "                                t = [tweetText, state, score, numberTerms]\n",
    "                                scoredTweets.append(t)\n",
    "                                \n",
    "                                if state in tweetsByStates:\n",
    "                                    tweetsByStates[state].append(score)\n",
    "                                else:\n",
    "                                    tweetsByStates[state] = [score]\n",
    "    print(tweetsByStates)\n",
    "    \n",
    "    with open('out.csv', mode='w',  newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow([\"state\", \"id\", \"meanSent\"])\n",
    "        \n",
    "        mean = None\n",
    "        for state in states_names:\n",
    "            lowerCaseState = state.lower()\n",
    "            if lowerCaseState in tweetsByStates:\n",
    "                mean = round(sum(tweetsByStates[lowerCaseState])/len(tweetsByStates[lowerCaseState]),2)\n",
    "            else: mean = 0\n",
    "            \n",
    "            writer.writerow([states_names[state], states_codes[state], mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ca': [2, 2, 3, 3], 'tx': [6, 7, -6, -3, -1, 2, 2, 3, 2, 4, -2], 'al': [1], 'oh': [-10, 2, 3, 1, 0, -3, -7], 'id': [2], 'ga': [-5, -4, -4, 2, 3], 'or': [2, -9, -4], 'fl': [1, -3, 3, 2, 1, 2, 3], 'ny': [3, -2, 3, -4, 1], 'mn': [1, -5, 1], 'pa': [3], 'nj': [2], 'mi': [-2], 'nd': [-3], 'ok': [4], 'md': [1], 'il': [1], 'tn': [-6], 'ma': [2], 'wa': [-8, 1], 'va': [-1, 4], 'in': [1], 'mo': [-1, -2], 'wv': [5], 'la': [1]}\n"
     ]
    }
   ],
   "source": [
    "analyzeUsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def isTweetInSapin(tweet):\n",
    "    return tweet[\"place\"] != None and tweet[\"place\"][\"country_code\"] == \"ES\"\n",
    "    \n",
    "def analyzeSpain():\n",
    "    file = \"inpSpain.txt\"\n",
    "    tweetsInSpainByLanguage = {}\n",
    "       \n",
    "    with open(file, \"r\", encoding='utf-16') as ins:\n",
    "        \n",
    "        for line in ins:          \n",
    "            line = line.strip(\"'<>() \").replace('\\'', '\\\"')\n",
    "            line = line.replace(\"b\\\"{\", \"{\")\n",
    "            line = line.replace(\"}\\\"\", \"}\")\n",
    "            line = line.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        \n",
    "            if len(line) > 1: ## to avoid empty lines \n",
    "                tweet = json.loads(line, encoding=\"utf-16\")\n",
    "           \n",
    "                if \"created_at\" in tweet:\n",
    "                    if isTweetInSapin(tweet):\n",
    "                        coordinates = tweet[\"place\"][\"bounding_box\"][\"coordinates\"][0][0]\n",
    "                        lang = tweet[\"lang\"]\n",
    "\n",
    "                        if coordinates != None and lang != None:\n",
    "                            if lang in tweetsInSpainByLanguage:\n",
    "                                tweetsInSpainByLanguage[lang].append(coordinates)\n",
    "                            else:\n",
    "                                tweetsInSpainByLanguage[lang] = [coordinates]\n",
    "    print(tweetsInSpainByLanguage)\n",
    "    \n",
    "    with open('outSpain.csv', mode='w',  newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        writer.writerow([\"lang\", \"latitude\", \"longitude\"])\n",
    "\n",
    "        for lang in tweetsInSpainByLanguage:\n",
    "            for coord in tweetsInSpainByLanguage[lang]:           \n",
    "                writer.writerow([lang, coord[1], coord[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'und': [[2.563782, 39.12507], [-7.005353, 37.107832], [-7.005353, 37.107832]], 'it': [[4.202715, 39.799157]], 'es': [[1.943749, 41.51714], [-6.0201, 43.278867], [-6.306111, 36.516968], [-8.471284, 43.301377], [-15.525504, 28.024813], [-3.889005, 40.312071], [-3.810999, 37.137275], [-3.037149, 43.319954], [2.086323, 41.336062], [-6.02843, 37.313613], [-3.889005, 40.312071], [-6.06368, 37.1481]], 'en': [[0.295386, 40.274335], [2.052477, 41.317048]], 'ca': [[2.446113, 42.150116]], 'tl': [[2.211092, 41.42594]]}\n"
     ]
    }
   ],
   "source": [
    "analyzeSpain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNotUsefulTweets():\n",
    "    file = \"inp.txt\"\n",
    "    \n",
    "    linesToKeep = []\n",
    "    \n",
    "    with open(file, \"r\", encoding='utf-16') as ins:\n",
    "        \n",
    "        for line in ins:          \n",
    "            cleanline = line.strip(\"'<>() \").replace('\\'', '\\\"')\n",
    "            cleanline = cleanline.replace(\"b\\\"{\", \"{\")\n",
    "            cleanline = cleanline.replace(\"}\\\"\", \"}\")\n",
    "            cleanline = cleanline.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        \n",
    "            if len(cleanline) > 1:\n",
    "                tweet = json.loads(cleanline, encoding=\"utf-16\")\n",
    "           \n",
    "                if \"created_at\" in tweet:\n",
    "                    state = getState(tweet)\n",
    "\n",
    "                    if isState(state):\n",
    "                        if \"text\" in tweet:                           \n",
    "                            tweetText = tweet[\"text\"]\n",
    "                            tokenizedTweet = nltk.word_tokenize(tweetText)\n",
    "                            \n",
    "                            score = 0\n",
    "                            numberTerms = 0\n",
    "                            \n",
    "                            numberTerms, score = analyzeEmotions(tokenizedTweet)\n",
    "                            \n",
    "                            if numberTerms > 0:\n",
    "                                linesToKeep.append(line)\n",
    "                                \n",
    "    with open(file, \"w\", encoding='utf-16') as ins:\n",
    "        for line in linesToKeep:\n",
    "            ins.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanNotUsefulTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNotUsefulTweetsSpain():\n",
    "    file = \"inpSpain.txt\"\n",
    "\n",
    "    linesToKeep = []\n",
    "       \n",
    "    with open(file, \"r\", encoding='utf-16') as ins:\n",
    "        \n",
    "        for line in ins:          \n",
    "            cleanline = line.strip(\"'<>() \").replace('\\'', '\\\"')\n",
    "            cleanline = cleanline.replace(\"b\\\"{\", \"{\")\n",
    "            cleanline = cleanline.replace(\"}\\\"\", \"}\")\n",
    "            cleanline = cleanline.replace(\"\\\\\\\\\", \"\\\\\")\n",
    "        \n",
    "            if len(cleanline) > 1:\n",
    "                tweet = json.loads(cleanline, encoding=\"utf-16\")\n",
    "           \n",
    "                if \"created_at\" in tweet:\n",
    "                    if isTweetInSapin(tweet):\n",
    "                        coordinates = tweet[\"place\"][\"bounding_box\"][\"coordinates\"][0][0]\n",
    "                        lang = tweet[\"lang\"]\n",
    "\n",
    "                        if coordinates != None and lang != None:\n",
    "                            linesToKeep.append(line)\n",
    "                            \n",
    "    with open(file, \"w\", encoding='utf-16') as ins:\n",
    "        for line in linesToKeep:\n",
    "            ins.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanNotUsefulTweetsSpain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
